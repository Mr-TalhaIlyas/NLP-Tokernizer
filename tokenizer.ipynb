{"cells":[{"cell_type":"code","execution_count":1,"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Jul 18 11:45:40 2022\n","\n","@author: talha\n","\"\"\"\n","#%%\n","import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow_datasets as tfds\n","import tensorflow_text as text\n","import tensorflow as tf\n","\n","tf.get_logger().setLevel('ERROR')\n","pwd = pathlib.Path.cwd()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# Load your data files\n","path2pt = 'C:/Users/talha/Desktop/tokenizer/ds/pt_train.txt'\n","path2en = 'C:/Users/talha/Desktop/tokenizer/ds/en_train.txt'\n","\n","pt_train = pathlib.Path(path2pt).read_text(encoding=\"utf-8\").splitlines()\n","en_train = pathlib.Path(path2en).read_text(encoding=\"utf-8\").splitlines()\n","\n","# convert the list to tensor\n","pt_train = tf.convert_to_tensor(pt_train, dtype=tf.string)\n","en_train = tf.convert_to_tensor(en_train, dtype=tf.string)\n","\n","# make dataset\n","pt_data = tf.data.Dataset.from_tensor_slices(pt_train)\n","en_data = tf.data.Dataset.from_tensor_slices(en_train)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# importing and initilize BERT tokenizer params\n","\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = 8000,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n","    )"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["'''\n","Generate vocab\n","'''\n","pt_vocab = bert_vocab.bert_vocab_from_dataset(\n","    pt_data.batch(1000).prefetch(2),\n","    **bert_vocab_args\n",")\n","\n","en_vocab = bert_vocab.bert_vocab_from_dataset(\n","    en_data.batch(1000).prefetch(2),\n","    **bert_vocab_args\n",")\n","# print some values in vocab\n","print(en_vocab[100:110])"],"outputs":[{"output_type":"stream","name":"stdout","text":["['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n"]}],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["'''\n","Wrtie vocab files\n","'''\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w', encoding=\"utf-8\") as f:\n","    for token in vocab:\n","      print(token, file=f)\n","      \n","write_vocab_file('pt_vocab.txt', pt_vocab)\n","write_vocab_file('en_vocab.txt', en_vocab)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["'''\n","give bert tokernizer the vocab of your dataset and initilize the tokenizer methods.\n","'''\n","pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\n","en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params) # bert_tokenizer_params=dict(lower_case=True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["'''\n","tokenize your text\n","'''\n","# get an exmple string from data and pass it to tokenizer\n","en_examples = []\n","for example in en_data.as_numpy_iterator():\n","    en_examples.append(example)\n","    break\n","\n","# Tokenize the examples -> (batch, word, word-piece)\n","token_batch = en_tokenizer.tokenize(en_examples) # 'hello there !' test it!!!\n","print(f'Token Batch shape : {token_batch.shape} => [batch, word, word-piece]')\n","# Merge the word and word-piece axes -> (batch, tokens)\n","token_batch = token_batch.merge_dims(-2,-1)\n","print(f'Token Batch reshaped : {token_batch.shape} => [batch, tokens]')\n","print(f'Tokenized list of input string : {token_batch.to_list}')"],"outputs":[{"output_type":"stream","name":"stdout","text":["Token Batch shape : (1, None, None) => [batch, word, word-piece]\n","Token Batch reshaped : (1, None) => [batch, tokens]\n","Tokenized list of input string : <bound method RaggedTensor.to_list of <tf.RaggedTensor [[2568, 101, 71, 56, 1548, 4593, 2159, 6437, 2364, 78, 2003, 93, 208, 67,\n","  78, 133, 74, 1332, 72, 638, 4039, 67, 71, 615, 75, 3458, 114, 190, 80,\n","  71, 2159, 6437, 893, 74, 730, 2654, 67, 108, 859, 73, 1510, 832, 2725,\n","  100, 83, 86, 13, 73, 164, 73, 71, 948, 74, 90, 83, 9, 105, 101, 95, 72,\n","  73, 281, 71, 679, 6258, 72, 73, 725, 93, 124, 15]]>>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["'''\n","detokenize your text\n","'''\n","words = en_tokenizer.detokenize(token_batch)\n","tf.strings.reduce_join(words, separator=' ', axis=-1)\n","print(f'Detokenized words: {words}')"],"outputs":[{"output_type":"stream","name":"stdout","text":["Detokenized words: <tf.RaggedTensor [[b'amongst', b'all', b'the', b'troubling', b'deficits', b'we',\n","  b'struggle', b'with', b'today', b'\\xe2\\x80\\x94', b'we', b'think', b'of',\n","  b'financial', b'and', b'economic', b'primarily', b'\\xe2\\x80\\x94',\n","  b'the', b'ones', b'that', b'concern', b'me', b'most', b'is', b'the',\n","  b'deficit', b'of', b'political', b'dialogue', b'\\xe2\\x80\\x94', b'our',\n","  b'ability', b'to', b'address', b'modern', b'conflicts', b'as', b'they',\n","  b'are', b',', b'to', b'go', b'to', b'the', b'source', b'of', b'what',\n","  b'they', b\"'\", b're', b'all', b'about', b'and', b'to', b'understand',\n","  b'the', b'key', b'players', b'and', b'to', b'deal', b'with', b'them',\n","  b'.']]>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["'''\n","But for transformer training we have to add the START and END sentense tokens so...\n","'''\n","START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  return tf.concat([starts, ragged, ends], axis=1)\n","\n","words = en_tokenizer.detokenize(add_start_end(token_batch))\n","tf.strings.reduce_join(words, separator=' ', axis=-1)\n","print(words)\n","'''\n","Also when we are printintg text we don't want START END tokens to be shown on the output so...\n","'''\n","def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result\n","\n","token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n","print(token_batch)\n","words = en_tokenizer.detokenize(add_start_end(token_batch))\n","print(words)\n","clean = cleanup_text(reserved_tokens, words).numpy()\n","print(clean)"],"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'[START]', b'amongst', b'all', b'the', b'troubling', b'deficits',\n","  b'we', b'struggle', b'with', b'today', b'\\xe2\\x80\\x94', b'we', b'think',\n","  b'of', b'financial', b'and', b'economic', b'primarily', b'\\xe2\\x80\\x94',\n","  b'the', b'ones', b'that', b'concern', b'me', b'most', b'is', b'the',\n","  b'deficit', b'of', b'political', b'dialogue', b'\\xe2\\x80\\x94', b'our',\n","  b'ability', b'to', b'address', b'modern', b'conflicts', b'as', b'they',\n","  b'are', b',', b'to', b'go', b'to', b'the', b'source', b'of', b'what',\n","  b'they', b\"'\", b're', b'all', b'about', b'and', b'to', b'understand',\n","  b'the', b'key', b'players', b'and', b'to', b'deal', b'with', b'them',\n","  b'.', b'[END]']]>\n","<tf.RaggedTensor [[2568, 101, 71, 56, 1548, 4593, 2159, 6437, 2364, 78, 2003, 93, 208, 67,\n","  78, 133, 74, 1332, 72, 638, 4039, 67, 71, 615, 75, 3458, 114, 190, 80,\n","  71, 2159, 6437, 893, 74, 730, 2654, 67, 108, 859, 73, 1510, 832, 2725,\n","  100, 83, 86, 13, 73, 164, 73, 71, 948, 74, 90, 83, 9, 105, 101, 95, 72,\n","  73, 281, 71, 679, 6258, 72, 73, 725, 93, 124, 15]]>\n","<tf.RaggedTensor [[b'[START]', b'amongst', b'all', b'the', b'troubling', b'deficits',\n","  b'we', b'struggle', b'with', b'today', b'\\xe2\\x80\\x94', b'we', b'think',\n","  b'of', b'financial', b'and', b'economic', b'primarily', b'\\xe2\\x80\\x94',\n","  b'the', b'ones', b'that', b'concern', b'me', b'most', b'is', b'the',\n","  b'deficit', b'of', b'political', b'dialogue', b'\\xe2\\x80\\x94', b'our',\n","  b'ability', b'to', b'address', b'modern', b'conflicts', b'as', b'they',\n","  b'are', b',', b'to', b'go', b'to', b'the', b'source', b'of', b'what',\n","  b'they', b\"'\", b're', b'all', b'about', b'and', b'to', b'understand',\n","  b'the', b'key', b'players', b'and', b'to', b'deal', b'with', b'them',\n","  b'.', b'[END]']]>\n","[b\"amongst all the troubling deficits we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the deficit of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they ' re all about and to understand the key players and to deal with them .\"]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path) # save the assets in the model\n","\n","    vocab = pathlib.Path(vocab_path).read_text(encoding=\"utf-8\").splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["'''\n","Initilize tf.Module and save the model by given params\n","'''\n","tokenizers = tf.Module()\n","tokenizers.pt = CustomTokenizer(reserved_tokens, 'pt_vocab.txt')\n","tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')\n","\n","model_name = 'my_pt_en_convertor'\n","tf.saved_model.save(tokenizers, model_name)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["'''\n","Relode for sanity check\n","'''\n","reloaded_tokenizers = tf.saved_model.load(model_name)\n","print(reloaded_tokenizers.en.get_vocab_size().numpy())\n","\n","tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n","print(tokens.numpy())\n","\n","text_tokens = reloaded_tokenizers.en.lookup(tokens)\n","print(text_tokens)\n","\n","round_trip = reloaded_tokenizers.en.detokenize(tokens)\n","\n","print(round_trip.numpy()[0].decode('utf-8'))"],"outputs":[{"output_type":"stream","name":"stdout","text":["7010\n","[[   2 4006 2358  687 1192 2365    4    3]]\n","<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n","  b'[END]']]>\n","hello tensorflow !\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}